# prepare_data.py
import os
import json
import random
from datasets import load_dataset

# 1) ≈πr√≥d≈Ça dialog√≥w
SOURCES = [
    ("bavard/personachat_truecased", {"split": "train", "trust_remote_code": True}),
    ("daily_dialog",               {"split": "train", "trust_remote_code": True}),
    ("facebook/empathetic_dialogues", {"split": "train"}),
    ("blended_skill_talk",         {"split": "train"}),
]

def extract_pairs(example):
    """
    Zwraca listƒô {"prompt":..., "completion":...} dla pojedynczego przyk≈Çadu.
    Obs≈Çuguje cztery schematy danych:
      - persona-chat truecased (pola "prompt" i "utterance")
      - daily_dialog    (pole "dialog": lista string√≥w)
      - empathetic_dialogues (pole "dialogue": lista dict√≥w {"utterance":...})
      - blended_skill_talk   (pole "utterances": lista string√≥w)
    """
    # --- PersonaChat truecased
    if "prompt" in example and "utterance" in example:
        return [{
            "prompt": example["prompt"] + "\nAI:",
            "completion": " " + example["utterance"]
        }]

    # --- pozosta≈Çe: ustalamy listƒô kolejnych wypowiedzi
    if "dialog" in example:
        dialogs = example["dialog"]
    elif "dialogue" in example:
        dialogs = [turn["utterance"] for turn in example["dialogue"]]
    elif "utterances" in example:
        dialogs = example["utterances"]
    else:
        return []

    pairs = []
    for i in range(len(dialogs) - 1):
        p = f"User: {dialogs[i]}\nAI:"
        c = " " + dialogs[i + 1]
        pairs.append({"prompt": p, "completion": c})
    return pairs

def main():
    all_pairs = []
    for repo_id, cfg in SOURCES:
        print(f"üîÑ ≈Åadujƒô {repo_id} ‚Ä¶")
        ds = load_dataset(repo_id, **cfg)
        for ex in ds:
            all_pairs.extend(extract_pairs(ex))

    print(f"‚úÖ Wygenerowano ≈ÇƒÖcznie {len(all_pairs)} par. Mieszam ‚Ä¶")
    random.seed(42)
    random.shuffle(all_pairs)

    split_at = int(0.8 * len(all_pairs))
    os.makedirs("data", exist_ok=True)
    with open("data/train.jsonl", "w", encoding="utf-8") as ftr, \
         open("data/valid.jsonl", "w", encoding="utf-8") as fval:
        for idx, pair in enumerate(all_pairs):
            line = json.dumps(pair, ensure_ascii=False)
            if idx < split_at:
                ftr.write(line + "\n")
            else:
                fval.write(line + "\n")

    print(f"‚úÖ Zapisano {split_at} rekord√≥w do data/train.jsonl i {len(all_pairs)-split_at} do data/valid.jsonl")

if __name__ == "__main__":
    main()
